ğŸŒŸ LLM Powered Multilingual Voice Assistant for Multimodal Data ğŸŒŸ


Welcome to a new era of AI-powered communication! This project is more than a voice assistantâ€”it's your multilingual, multimodal companion that understands text, speech, and even images. Built with state-of-the-art AI models and optimized for real-time performance, this assistant speaks your language and adapts to your needs.

ğŸš€ What Can It Do?

1. Speak and Understand Any Language
Ask a question in Hindi, get the answer in Hindi.
Speak in Mandarin, hear the response in Mandarin.

2. See and Process Images
Upload an image, and it extracts and interprets the text or context.
Example: Upload a restaurant menu and ask, "What are the vegetarian options?"
ğŸŒ Supports any language you can think of!

3. See and Process Images
Upload an image, and it extracts and interprets the text or context.
Example: Upload a restaurant menu and ask, "What are the vegetarian options?"

4. Intelligent and Contextual Responses
Ask anything, and it gives meaningful answers using the Llava 1.5 7B model for multimodal data understanding.

5. Multimodal Magic
Combines speech, text, and images for a seamless user experience.

6. Natural and Clear Voices
Responds with realistic, natural-sounding voices using gTTS, maintaining the same language and tone as the query.

ğŸ› ï¸ Tech Stack & Innovation


Technology	Role in the Project


Llava 1.5 7B-	Multimodal AI for text and image understanding


Whisper by OpenAI-	Multilingual speech-to-text processing


gTTS-	Converts text to natural audio responses


Gradio-	Provides an intuitive web-based user interface


CUDA (PyTorch)-	Accelerates performance with GPU-powered speed

BitsAndBytes-	Enables 4-bit quantization for memory efficiency

ğŸ¥ How It Works (Step-by-Step)
Input Your Query

Speak, type, or upload an image.
The assistant processes your input in real-time, whether it's in text, speech, or image form.
Intelligent Processing

Speech-to-Text: Converts spoken queries into text using Whisper.
Image/Text Understanding: Extracts context and generates responses with Llava 1.5 7B.
Generate a Response

Combines multimodal data to generate smart, context-aware answers.
Speak Back

Outputs the response as natural-sounding audio in your preferred language using gTTS.


ğŸŒ Why Choose This Assistant?
True Multilingual Support: Speak any language, and it speaks back in the same.

Multimodal Brilliance: Combines text, speech, and images into one powerful assistant.

Real-Time Interaction: Powered by CUDA for instant responses.

Customizable MVP: A blueprint for your very own AI assistant.

ğŸ’¡ Example Use Cases
ğŸŒ Global Communication
A French-speaking user asks:
"Quelle est la mÃ©tÃ©o aujourd'hui ?"
The assistant responds in French, both in text and audio.


ğŸ–¼ï¸ Image Understanding
Upload a picture of a recipe and ask:
"What are the ingredients?"
The assistant extracts and explains!


ğŸ› ï¸ Hobby Project or Business MVP
Build and demo your very own assistant for business solutions, accessibility, or education.

Output Generation

Speech-to-Text Conversion:
The user's speech is accurately transcribed into text in the same language using Whisper.

Response Generation:
Based on the input, the assistant uses the Llava 1.5 7B model to generate intelligent and contextually relevant responses.

Text-to-Speech Output:
The response text is converted into spoken output using gTTS, maintaining the language and tone of the original query.
Example: If the user asked a question in Spanish, the response is also spoken in Spanish with proper pronunciation.If it has any fallback then it answers in english

ğŸš€ Getting Started
Prerequisites
Python 3.8+ installed.
CUDA-enabled GPU for faster processing.



ğŸ¤ Contributions
This is a personal project by Anjali Kabra, but feedback and suggestions are always welcome.

ğŸ“œ License
Licensed under the MIT License. See LICENSE for more details.
